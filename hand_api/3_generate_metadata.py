import os
import glob
import json
import argparse
import pandas as pd
import numpy as np
from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡

def compute_action_statistics(actions_data):
    """
    è®¡ç®—å•ä¸ªepisodeçš„åŠ¨ä½œç»Ÿè®¡å€¼
    
    Args:
        actions_data: åŒ…å«åŠ¨ä½œæ•°æ®çš„numpyæ•°ç»„
        
    Returns:
        åŒ…å«ç»Ÿè®¡å€¼çš„å­—å…¸
    """
    if actions_data is None or len(actions_data) == 0:
        return None
    
    # åŠ¨ä½œæ•°æ®æ ¼å¼: [trans_x, trans_y, trans_z, rot_r, rot_p, rot_y, open]
    # åˆ†åˆ«è®¡ç®—æ¯ä¸ªç»´åº¦çš„ç»Ÿè®¡å€¼
    
    stats = {
        "num_frames": len(actions_data),
        "translation": {},
        "rotation": {},
        "gripper": {},
        "overall": {}
    }
    
    # å¹³ç§»ç»Ÿè®¡ (å‰3åˆ—: x, y, z)
    if actions_data.shape[1] >= 3:
        trans_data = actions_data[:, :3]
        stats["translation"] = {
            "x": {
                "mean": float(np.mean(trans_data[:, 0])),
                "std": float(np.std(trans_data[:, 0])),
                "min": float(np.min(trans_data[:, 0])),
                "max": float(np.max(trans_data[:, 0])),
                "range": float(np.max(trans_data[:, 0]) - np.min(trans_data[:, 0]))
            },
            "y": {
                "mean": float(np.mean(trans_data[:, 1])),
                "std": float(np.std(trans_data[:, 1])),
                "min": float(np.min(trans_data[:, 1])),
                "max": float(np.max(trans_data[:, 1])),
                "range": float(np.max(trans_data[:, 1]) - np.min(trans_data[:, 1]))
            },
            "z": {
                "mean": float(np.mean(trans_data[:, 2])),
                "std": float(np.std(trans_data[:, 2])),
                "min": float(np.min(trans_data[:, 2])),
                "max": float(np.max(trans_data[:, 2])),
                "range": float(np.max(trans_data[:, 2]) - np.min(trans_data[:, 2]))
            }
        }
        
        # è®¡ç®—å¹³ç§»å¹…åº¦
        trans_magnitudes = np.linalg.norm(trans_data, axis=1)
        stats["translation"]["magnitude"] = {
            "mean": float(np.mean(trans_magnitudes)),
            "std": float(np.std(trans_magnitudes)),
            "min": float(np.min(trans_magnitudes)),
            "max": float(np.max(trans_magnitudes)),
            "range": float(np.max(trans_magnitudes) - np.min(trans_magnitudes))
        }
    
    # æ—‹è½¬ç»Ÿè®¡ (ä¸­é—´3åˆ—: roll, pitch, yaw)
    if actions_data.shape[1] >= 6:
        rot_data = actions_data[:, 3:6]
        stats["rotation"] = {
            "roll": {
                "mean": float(np.mean(rot_data[:, 0])),
                "std": float(np.std(rot_data[:, 0])),
                "min": float(np.min(rot_data[:, 0])),
                "max": float(np.max(rot_data[:, 0])),
                "range": float(np.max(rot_data[:, 0]) - np.min(rot_data[:, 0]))
            },
            "pitch": {
                "mean": float(np.mean(rot_data[:, 1])),
                "std": float(np.std(rot_data[:, 1])),
                "min": float(np.min(rot_data[:, 1])),
                "max": float(np.max(rot_data[:, 1])),
                "range": float(np.max(rot_data[:, 1]) - np.min(rot_data[:, 1]))
            },
            "yaw": {
                "mean": float(np.mean(rot_data[:, 2])),
                "std": float(np.std(rot_data[:, 2])),
                "min": float(np.min(rot_data[:, 2])),
                "max": float(np.max(rot_data[:, 2])),
                "range": float(np.max(rot_data[:, 2]) - np.min(rot_data[:, 2]))
            }
        }
        
        # è®¡ç®—æ—‹è½¬å¹…åº¦
        rot_magnitudes = np.linalg.norm(rot_data, axis=1)
        stats["rotation"]["magnitude"] = {
            "mean": float(np.mean(rot_magnitudes)),
            "std": float(np.std(rot_magnitudes)),
            "min": float(np.min(rot_magnitudes)),
            "max": float(np.max(rot_magnitudes)),
            "range": float(np.max(rot_magnitudes) - np.min(rot_magnitudes))
        }
    
    # å¤¹çˆªç»Ÿè®¡ (æœ€åä¸€åˆ—: open/close)
    if actions_data.shape[1] >= 7:
        gripper_data = actions_data[:, 6]
        stats["gripper"] = {
            "mean": float(np.mean(gripper_data)),
            "std": float(np.std(gripper_data)),
            "min": float(np.min(gripper_data)),
            "max": float(np.max(gripper_data)),
            "range": float(np.max(gripper_data) - np.min(gripper_data)),
            "open_frames": int(np.sum(gripper_data > 50)),  # å‡è®¾>50ä¸ºæ‰“å¼€çŠ¶æ€
            "close_frames": int(np.sum(gripper_data <= 50))  # å‡è®¾<=50ä¸ºå…³é—­çŠ¶æ€
        }
    
    # æ•´ä½“åŠ¨ä½œç»Ÿè®¡
    if actions_data.shape[1] >= 7:
        # è®¡ç®—åŠ¨ä½œå˜åŒ–ç‡ (ç›¸é‚»å¸§ä¹‹é—´çš„å·®å¼‚)
        action_diffs = np.diff(actions_data, axis=0)
        action_change_magnitudes = np.linalg.norm(action_diffs, axis=1)
        
        stats["overall"] = {
            "total_action_change": float(np.sum(action_change_magnitudes)),
            "mean_action_change": float(np.mean(action_change_magnitudes)),
            "max_action_change": float(np.max(action_change_magnitudes)),
            "action_smoothness": float(1.0 / (1.0 + np.mean(action_change_magnitudes)))  # å¹³æ»‘åº¦æŒ‡æ ‡
        }
    
    return stats

def compute_dataset_summary_statistics(all_episode_stats):
    """
    è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡æ‘˜è¦
    
    Args:
        all_episode_stats: æ‰€æœ‰episodeçš„ç»Ÿè®¡å€¼å­—å…¸
        
    Returns:
        åŒ…å«æ•°æ®é›†æ•´ä½“ç»Ÿè®¡æ‘˜è¦çš„å­—å…¸
    """
    if not all_episode_stats:
        return {}
    
    # æ”¶é›†æ‰€æœ‰episodeçš„ç»Ÿè®¡å€¼
    all_trans_x_means = []
    all_trans_y_means = []
    all_trans_z_means = []
    all_rot_r_means = []
    all_rot_p_means = []
    all_rot_y_means = []
    all_gripper_means = []
    all_frame_counts = []
    all_action_smoothness = []
    
    for episode_name, stats in all_episode_stats.items():
        if "translation" in stats and "x" in stats["translation"]:
            all_trans_x_means.append(stats["translation"]["x"]["mean"])
            all_trans_y_means.append(stats["translation"]["y"]["mean"])
            all_trans_z_means.append(stats["translation"]["z"]["mean"])
        
        if "rotation" in stats and "roll" in stats["rotation"]:
            all_rot_r_means.append(stats["rotation"]["roll"]["mean"])
            all_rot_p_means.append(stats["rotation"]["pitch"]["mean"])
            all_rot_y_means.append(stats["rotation"]["yaw"]["mean"])
        
        if "gripper" in stats:
            all_gripper_means.append(stats["gripper"]["mean"])
        
        if "num_frames" in stats:
            all_frame_counts.append(stats["num_frames"])
        
        if "overall" in stats and "action_smoothness" in stats["overall"]:
            all_action_smoothness.append(stats["overall"]["action_smoothness"])
    
    # è®¡ç®—æ•´ä½“ç»Ÿè®¡æ‘˜è¦
    summary = {
        "total_episodes": len(all_episode_stats),
        "frame_statistics": {
            "total_frames": sum(all_frame_counts) if all_frame_counts else 0,
            "mean_frames_per_episode": float(np.mean(all_frame_counts)) if all_frame_counts else 0,
            "std_frames_per_episode": float(np.std(all_frame_counts)) if all_frame_counts else 0,
            "min_frames": min(all_frame_counts) if all_frame_counts else 0,
            "max_frames": max(all_frame_counts) if all_frame_counts else 0
        }
    }
    
    # å¹³ç§»ç»Ÿè®¡æ‘˜è¦
    if all_trans_x_means:
        summary["translation_summary"] = {
            "x": {
                "mean": float(np.mean(all_trans_x_means)),
                "std": float(np.std(all_trans_x_means)),
                "min": float(np.min(all_trans_x_means)),
                "max": float(np.max(all_trans_x_means))
            },
            "y": {
                "mean": float(np.mean(all_trans_y_means)),
                "std": float(np.std(all_trans_y_means)),
                "min": float(np.min(all_trans_y_means)),
                "max": float(np.max(all_trans_y_means))
            },
            "z": {
                "mean": float(np.mean(all_trans_z_means)),
                "std": float(np.std(all_trans_z_means)),
                "min": float(np.min(all_trans_z_means)),
                "max": float(np.max(all_trans_z_means))
            }
        }
    
    # æ—‹è½¬ç»Ÿè®¡æ‘˜è¦
    if all_rot_r_means:
        summary["rotation_summary"] = {
            "roll": {
                "mean": float(np.mean(all_rot_r_means)),
                "std": float(np.std(all_rot_r_means)),
                "min": float(np.min(all_rot_r_means)),
                "max": float(np.max(all_rot_r_means))
            },
            "pitch": {
                "mean": float(np.mean(all_rot_p_means)),
                "std": float(np.std(all_rot_p_means)),
                "min": float(np.min(all_rot_p_means)),
                "max": float(np.max(all_rot_p_means))
            },
            "yaw": {
                "mean": float(np.mean(all_rot_y_means)),
                "std": float(np.std(all_rot_y_means)),
                "min": float(np.min(all_rot_y_means)),
                "max": float(np.max(all_rot_y_means))
            }
        }
    
    # å¤¹çˆªç»Ÿè®¡æ‘˜è¦
    if all_gripper_means:
        summary["gripper_summary"] = {
            "mean": float(np.mean(all_gripper_means)),
            "std": float(np.std(all_gripper_means)),
            "min": float(np.min(all_gripper_means)),
            "max": float(np.max(all_gripper_means))
        }
    
    # åŠ¨ä½œå¹³æ»‘åº¦æ‘˜è¦
    if all_action_smoothness:
        summary["action_smoothness_summary"] = {
            "mean": float(np.mean(all_action_smoothness)),
            "std": float(np.std(all_action_smoothness)),
            "min": float(np.min(all_action_smoothness)),
            "max": float(np.max(all_action_smoothness))
        }
    
    return summary

# --- CLI å‚æ•° ---
parser = argparse.ArgumentParser(description="Generate LeRobot v2 metadata (episodes/info/stats/action_statistics/modality)")
parser.add_argument(
    "--dataset-path",
    type=str,
    required=True,
    help="æ•°æ®é›†æ ¹ç›®å½•ï¼ˆåŒ…å« data/ meta/ videos/ ç­‰å­ç›®å½•çš„æ–‡ä»¶å¤¹ï¼‰",
)
parser.add_argument(
    "--chunk-name",
    type=str,
    default="chunk-000",
    help="æ•°æ® chunk åç§°ï¼ˆé»˜è®¤: chunk-000ï¼‰",
)
args = parser.parse_args()

# --- è·¯å¾„æ„å»º ---
dataset_path = args.dataset_path
chunk_name = args.chunk_name
data_path = os.path.join(dataset_path, "data", chunk_name)
meta_path = os.path.join(dataset_path, "meta")

# --- æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨ ---
if not os.path.isdir(data_path):
    print(f"é”™è¯¯ï¼šæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
    exit()

# --- åˆ›å»º meta ç›®å½• ---
os.makedirs(meta_path, exist_ok=True)
print(f"å…ƒæ•°æ®ç›®å½• '{meta_path}' å·²ç¡®ä¿å­˜åœ¨ã€‚")

# --- æŸ¥æ‰¾ Parquet æ–‡ä»¶ ---
# ä½¿ç”¨ glob æŸ¥æ‰¾æ‰€æœ‰ episode æ–‡ä»¶å¹¶æ’åºï¼Œç¡®ä¿ episode_index æŒ‰é¡ºåºå¤„ç†
parquet_files = sorted(glob.glob(os.path.join(data_path, "episode_*.parquet")))

if not parquet_files:
    print(f"é”™è¯¯ï¼šåœ¨ {data_path} ä¸­æ²¡æœ‰æ‰¾åˆ° episode_*.parquet æ–‡ä»¶ã€‚")
    exit()

print(f"æ‰¾åˆ° {len(parquet_files)} ä¸ª episode Parquet æ–‡ä»¶ã€‚")

# --- åˆå§‹åŒ–å˜é‡ ---
total_episodes = 0
total_frames = 0
episodes_data = [] # ç”¨äº episodes.jsonl
tasks_data = []    # ç”¨äº tasks.jsonl
action_stats = {"min": None, "max": None, "mean": None, "std": None, "dim": None, "q01": None, "q99": None}
state_stats = {"min": None, "max": None, "mean": None, "std": None, "dim": None, "q99": None}
first_action_shape = None # ç”¨äºè·å–ç»´åº¦
first_state_shape = None # ç”¨äºè·å–stateç»´åº¦

# ç”¨äºæ”¶é›†æ‰€æœ‰æ•°æ®è¿›è¡Œç»Ÿè®¡è®¡ç®—
all_actions = []
all_states = []

# ç”¨äºæ”¶é›†æ‰€æœ‰episodeçš„åŠ¨ä½œç»Ÿè®¡å€¼
all_episode_stats = {}

# --- å¤„ç†æ¯ä¸ª Parquet æ–‡ä»¶ ---
print("æ­£åœ¨å¤„ç† Parquet æ–‡ä»¶å¹¶ç”Ÿæˆå…ƒæ•°æ®...")
for file_path in tqdm(parquet_files, desc="å¤„ç† Episodes"):
    try:
        # æå– episode_index
        filename = os.path.basename(file_path)
        episode_index_str = filename.replace("episode_", "").replace(".parquet", "")
        episode_index = int(episode_index_str)
        print("file_path", file_path)
        # è¯»å– Parquet æ–‡ä»¶
        df = pd.read_parquet(file_path)
        print("df", df.head())
        print("df.columns", df.columns)
        print("df['action'].shape", df['action'].shape)

        # è·å–è½¨è¿¹é•¿åº¦
        length = len(df)
        print("length", length)

        # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
        total_episodes += 1
        total_frames += length

        # --- å‡†å¤‡ episodes.jsonl æ•°æ® ---
        # å‡è®¾ä»»åŠ¡åç§°å›ºå®šä¸º "pick_put"
        episode_info = {
            "episode_index": episode_index,
            "tasks": ["[Robot] pull the drawer and then push the drawer."], # æ ¹æ®ä½ çš„ä¾‹å­
            "length": length
        }
        episodes_data.append(episode_info)

        # --- å‡†å¤‡ tasks.jsonl æ•°æ® ---
        # å‡è®¾ä»»åŠ¡åç§°ä¸º "pick_put"ï¼ŒID ä¸º 0
        task_info = {
             "episode_index": episode_index,
             "task_name": "pick_put",
             "task_id": 0 # å‡è®¾åªæœ‰ä¸€ä¸ªä»»åŠ¡ï¼ŒIDä¸º0
        }
        tasks_data.append(task_info)

        # --- æ”¶é›†actionå’Œstateæ•°æ®ç”¨äºstats.json ---
        # å¤„ç†actionæ•°æ®
        if 'action' in df.columns and len(df) > 0:
            try:
                # è·å–actionæ•°æ®
                episode_actions = []
                for idx, action_data in enumerate(df['action']):
                    if isinstance(action_data, (list, np.ndarray)):
                        episode_actions.append(np.array(action_data, dtype=np.float32))
                    elif isinstance(action_data, str):
                        try:
                            parsed_action = json.loads(action_data.replace("'", "\""))
                            if isinstance(parsed_action, list):
                                episode_actions.append(np.array(parsed_action, dtype=np.float32))
                        except json.JSONDecodeError:
                            print(f"è­¦å‘Šï¼šæ— æ³•è§£æ episode {episode_index} ç¬¬ {idx} å¸§çš„åŠ¨ä½œå­—ç¬¦ä¸²ã€‚")
                            continue

                if episode_actions:
                    episode_actions = np.array(episode_actions)
                    all_actions.append(episode_actions)
                    
                    # è·å–ç¬¬ä¸€ä¸ªactionçš„ç»´åº¦ä¿¡æ¯
                    if first_action_shape is None:
                        first_action_shape = episode_actions[0].shape
                        print(f"ä» episode {episode_index} è·å–åˆ°åŠ¨ä½œç»´åº¦: {first_action_shape}")
                    
                    # è®¡ç®—è¯¥episodeçš„åŠ¨ä½œç»Ÿè®¡å€¼
                    episode_stats = compute_action_statistics(episode_actions)
                    if episode_stats:
                        episode_name = f"episode_{episode_index:06d}"
                        all_episode_stats[episode_name] = episode_stats
                        print(f"  ğŸ“Š è®¡ç®—äº† {episode_name} çš„åŠ¨ä½œç»Ÿè®¡å€¼")

            except Exception as e:
                print(f"è­¦å‘Šï¼šå¤„ç† episode {episode_index} çš„actionæ•°æ®æ—¶å‡ºé”™: {e}")

        # å¤„ç†observation.stateæ•°æ®
        if 'observation.state' in df.columns and len(df) > 0:
            try:
                # è·å–stateæ•°æ®
                episode_states = []
                for idx, state_data in enumerate(df['observation.state']):
                    if isinstance(state_data, (list, np.ndarray)):
                        episode_states.append(np.array(state_data, dtype=np.float32))
                    elif isinstance(state_data, str):
                        try:
                            parsed_state = json.loads(state_data.replace("'", "\""))
                            if isinstance(parsed_state, list):
                                episode_states.append(np.array(parsed_state, dtype=np.float32))
                        except json.JSONDecodeError:
                            print(f"è­¦å‘Šï¼šæ— æ³•è§£æ episode {episode_index} ç¬¬ {idx} å¸§çš„çŠ¶æ€å­—ç¬¦ä¸²ã€‚")
                            continue
                
                if episode_states:
                    episode_states = np.array(episode_states)
                    all_states.append(episode_states)
                    
                    # è·å–ç¬¬ä¸€ä¸ªstateçš„ç»´åº¦ä¿¡æ¯
                    if first_state_shape is None:
                        first_state_shape = episode_states[0].shape
                        print(f"ä» episode {episode_index} è·å–åˆ°çŠ¶æ€ç»´åº¦: {first_state_shape}")
                        
            except Exception as e:
                print(f"è­¦å‘Šï¼šå¤„ç† episode {episode_index} çš„stateæ•°æ®æ—¶å‡ºé”™: {e}")


    except Exception as e:
        print(f"\nå¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        continue # è·³è¿‡æœ‰é—®é¢˜çš„æ–‡ä»¶

# --- å†™å…¥ episodes.jsonl ---
episodes_jsonl_path = os.path.join(meta_path, "episodes.jsonl")
try:
    with open(episodes_jsonl_path, 'w', encoding='utf-8') as f:
        for item in episodes_data:
            # ç¡®ä¿å†™å…¥çš„æ˜¯ç´§å‡‘çš„ JSON å­—ç¬¦ä¸²ï¼Œå¹¶æ·»åŠ æ¢è¡Œç¬¦
            json.dump(item, f, separators=(',', ':'))
            f.write('\n')
    print(f"æˆåŠŸå†™å…¥: {episodes_jsonl_path}")
except Exception as e:
    print(f"å†™å…¥ {episodes_jsonl_path} æ—¶å‡ºé”™: {e}")

# # --- å†™å…¥ tasks.jsonl ---
# tasks_jsonl_path = os.path.join(meta_path, "tasks.jsonl")
# try:
#     with open(tasks_jsonl_path, 'w', encoding='utf-8') as f:
#         for item in tasks_data:
#             json.dump(item, f, separators=(',', ':'))
#             f.write('\n')
#     print(f"æˆåŠŸå†™å…¥: {tasks_jsonl_path}")
# except Exception as e:
#     print(f"å†™å…¥ {tasks_jsonl_path} æ—¶å‡ºé”™: {e}")

# --- å†™å…¥ info.json ---
info_data = {
    "codebase_version": "v2.0",
    "robot_type": "so100",
    "total_episodes": total_episodes,
    "total_frames": total_frames,
    "total_tasks": 1,
    "total_videos": 2*total_episodes,
    "total_chunks": 1,
    "chunks_size": 1000,
    "fps": 30,
    "splits": {
        "train": f"0:{total_episodes}"
    },
    "data_path": "data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet",
    "video_path": "videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4",
    "features": {
        "action": {
            "dtype": "float32",
            "shape": [7],
            "names": [
                "right_shoulder_rotation_joint",
                "right_shoulder_pitch_joint",
                "right_ellbow_joint",
                "right_wrist_pitch_joint",
                "right_wrist_jaw_joint",
                "right_wrist_roll_joint",
                "right_gripper_joint"
            ]
        },
        "observation.state": {
            "dtype": "float32",
            "shape": [7],
            "names": [
                "right_shoulder_rotation_joint",
                "right_shoulder_pitch_joint",
                "right_ellbow_joint",
                "right_wrist_pitch_joint",
                "right_wrist_jaw_joint",
                "right_wrist_roll_joint",
                "right_gripper_joint"
            ]
        },
        "observation.images.webcam": {
            "dtype": "video",
            "shape": [480, 640, 3],
            "names": ["height", "width", "channels"],
            "info": {
                "video.fps": 30.0,
                "video.height": 480,
                "video.width": 640,
                "video.channels": 3,
                "video.codec": "av1",
                "video.pix_fmt": "yuv420p",
                "video.is_depth_map": False,
                "has_audio": False
            }
        },
        "observation.images.phone": {
            "dtype": "video",
            "shape": [480, 640, 3],
            "names": ["height", "width", "channels"],
            "info": {
                "video.fps": 30.0,
                "video.height": 480,
                "video.width": 640,
                "video.channels": 3,
                "video.codec": "av1",
                "video.pix_fmt": "yuv420p",
                "video.is_depth_map": False,
                "has_audio": False
            }
        },
        "timestamp": {
            "dtype": "float32",
            "shape": [1],
            "names": None
        },
        "frame_index": {
            "dtype": "int64",
            "shape": [1],
            "names": None
        },
        "episode_index": {
            "dtype": "int64",
            "shape": [1],
            "names": None
        },
        "index": {
            "dtype": "int64",
            "shape": [1],
            "names": None
        },
        "task_index": {
            "dtype": "int64",
            "shape": [1],
            "names": None
        }
    }
}
print(info_data)
info_json_path = os.path.join(meta_path, "info.json")
try:
    with open(info_json_path, 'w', encoding='utf-8') as f:
        json.dump(info_data, f, indent=4) # ä½¿ç”¨ indent ä½¿å…¶æ›´æ˜“è¯»
    print(f"æˆåŠŸå†™å…¥: {info_json_path}")
    # æ‰“å°è®¡ç®—å‡ºçš„ç»Ÿè®¡ä¿¡æ¯
    print("\n--- è®¡ç®—å¾—åˆ°çš„ç»Ÿè®¡ä¿¡æ¯ ---")
    print(f"æ€» Episodes (total_episodes): {info_data['total_episodes']}")
    print(f"æ€»å¸§æ•° (total_frames): {info_data['total_frames']}")
    print(f"æ€»è§†é¢‘æ•° (total_videos): {info_data['total_videos']}")
except Exception as e:
    print(f"å†™å…¥ {info_json_path} æ—¶å‡ºé”™: {e}")


# --- è®¡ç®—ç»Ÿè®¡ä¿¡æ¯å¹¶å†™å…¥ stats.json ---
print("\næ­£åœ¨è®¡ç®—actionå’Œstateçš„ç»Ÿè®¡ä¿¡æ¯...")

def compute_stats(arr, shape):
    stats = {}
    stats["mean"] = np.mean(arr, axis=0).tolist()
    stats["std"] = np.std(arr, axis=0).tolist()
    stats["min"] = np.min(arr, axis=0).tolist()
    stats["max"] = np.max(arr, axis=0).tolist()
    stats["q01"] = np.percentile(arr, 1, axis=0).tolist()
    stats["q99"] = np.percentile(arr, 99, axis=0).tolist()
    stats["dim"] = list(shape) if shape else None
    return stats

# è®¡ç®—actionç»Ÿè®¡ä¿¡æ¯
if all_actions:
    try:
        # å°†æ‰€æœ‰episodeçš„actionæ•°æ®åˆå¹¶
        all_actions_concat = np.concatenate(all_actions, axis=0)  # [total_frames, action_dim]
        print(f"Actionæ•°æ®æ€»å¸§æ•°: {all_actions_concat.shape[0]}, ç»´åº¦: {all_actions_concat.shape[1]}")
        action_stats = compute_stats(all_actions_concat, first_action_shape)
        print(f"Actionç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆï¼Œç»´åº¦: {action_stats['dim']}")
    except Exception as e:
        print(f"è®¡ç®—actionç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        action_stats = {"min": None, "max": None, "mean": None, "std": None, "dim": None, "q01": None, "q99": None}
else:
    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•actionæ•°æ®")

# è®¡ç®—stateç»Ÿè®¡ä¿¡æ¯
if all_states:
    try:
        all_states_concat = np.concatenate(all_states, axis=0)
        print(f"Stateæ•°æ®æ€»å¸§æ•°: {all_states_concat.shape[0]}, ç»´åº¦: {all_states_concat.shape[1]}")
        state_stats = compute_stats(all_states_concat, first_state_shape)
        print(f"Stateç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆï¼Œç»´åº¦: {state_stats['dim']}")
    except Exception as e:
        print(f"è®¡ç®—stateç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        state_stats = {"min": None, "max": None, "mean": None, "std": None, "dim": None, "q01": None, "q99": None}
else:
    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•stateæ•°æ®")

# æ„å»ºstatsæ•°æ®ç»“æ„
stats_data = {
    "action": {k: v for k, v in action_stats.items() if k != "dim"},
    "observation.state": {k: v for k, v in state_stats.items() if k != "dim"}
}

stats_json_path = os.path.join(meta_path, "stats.json")
try:
    with open(stats_json_path, 'w', encoding='utf-8') as f:
        def numpy_encoder(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        json.dump(stats_data, f, indent=4, default=numpy_encoder)
    print(f"æˆåŠŸå†™å…¥: {stats_json_path}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯æ‘˜è¦
    print("\n--- Actionç»Ÿè®¡ä¿¡æ¯æ‘˜è¦ ---")
    if action_stats["dim"]:
        print(f"ç»´åº¦: {action_stats['dim']}")
        print(f"å‡å€¼èŒƒå›´: {min(action_stats['mean']):.6f} ~ {max(action_stats['mean']):.6f}")
        print(f"æ ‡å‡†å·®èŒƒå›´: {min(action_stats['std']):.6f} ~ {max(action_stats['std']):.6f}")
        print(f"1%åˆ†ä½æ•°èŒƒå›´: {min(action_stats['q01']):.6f} ~ {max(action_stats['q01']):.6f}")
        print(f"99%åˆ†ä½æ•°èŒƒå›´: {min(action_stats['q99']):.6f} ~ {max(action_stats['q99']):.6f}")
    else:
        print("æ— Actionç»Ÿè®¡æ•°æ®")

    print("\n--- Stateç»Ÿè®¡ä¿¡æ¯æ‘˜è¦ ---")
    if state_stats["dim"]:
        print(f"ç»´åº¦: {state_stats['dim']}")
        print(f"å‡å€¼èŒƒå›´: {min(state_stats['mean']):.6f} ~ {max(state_stats['mean']):.6f}")
        print(f"æ ‡å‡†å·®èŒƒå›´: {min(state_stats['std']):.6f} ~ {max(state_stats['std']):.6f}")
        print(f"1%åˆ†ä½æ•°èŒƒå›´: {min(state_stats['q01']):.6f} ~ {max(state_stats['q01']):.6f}")
        print(f"99%åˆ†ä½æ•°èŒƒå›´: {min(state_stats['q99']):.6f} ~ {max(state_stats['q99']):.6f}")
    else:
        print("æ— Stateç»Ÿè®¡æ•°æ®")

except Exception as e:
    print(f"å†™å…¥ {stats_json_path} æ—¶å‡ºé”™: {e}")

# --- ç”ŸæˆåŠ¨ä½œç»Ÿè®¡å€¼JSONæ–‡ä»¶ ---
if all_episode_stats:
    print("\næ­£åœ¨ç”ŸæˆåŠ¨ä½œç»Ÿè®¡å€¼æ–‡ä»¶...")
    
    # è®¡ç®—æ•°æ®é›†æ•´ä½“ç»Ÿè®¡æ‘˜è¦
    dataset_summary_stats = compute_dataset_summary_statistics(all_episode_stats)
    
    # æ„å»ºåŠ¨ä½œç»Ÿè®¡æ•°æ®ç»“æ„
    action_statistics_data = {
        "dataset_info": {
            "dataset_path": dataset_path,
            "dataset_name": os.path.basename(os.path.normpath(dataset_path)),
            "total_episodes": total_episodes,
            "total_frames": total_frames,
            "processing_timestamp": pd.Timestamp.now().isoformat()
        },
        "dataset_summary": dataset_summary_stats,
        "episode_statistics": all_episode_stats
    }
    
    # ä¿å­˜åŠ¨ä½œç»Ÿè®¡å€¼åˆ°JSONæ–‡ä»¶
    action_stats_path = os.path.join(meta_path, "action_statistics.json")
    try:
        with open(action_stats_path, 'w', encoding='utf-8') as f:
            json.dump(action_statistics_data, f, indent=2, ensure_ascii=False)
        print(f"âœ“ åŠ¨ä½œç»Ÿè®¡å€¼å·²ä¿å­˜åˆ°: {action_stats_path}")
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print(f"\n--- åŠ¨ä½œç»Ÿè®¡æ‘˜è¦ ---")
        print(f"æ€»episodeæ•°: {dataset_summary_stats.get('total_episodes', 0)}")
        print(f"æ€»å¸§æ•°: {dataset_summary_stats.get('frame_statistics', {}).get('total_frames', 0)}")
        print(f"å¹³å‡å¸§æ•°/episode: {dataset_summary_stats.get('frame_statistics', {}).get('mean_frames_per_episode', 0):.1f}")
        
        if "gripper_summary" in dataset_summary_stats:
            gripper_summary = dataset_summary_stats["gripper_summary"]
            print(f"å¤¹çˆªå¹³å‡å€¼èŒƒå›´: [{gripper_summary['min']:.1f}, {gripper_summary['max']:.1f}]")
        
        if "action_smoothness_summary" in dataset_summary_stats:
            smoothness_summary = dataset_summary_stats["action_smoothness_summary"]
            print(f"åŠ¨ä½œå¹³æ»‘åº¦èŒƒå›´: [{smoothness_summary['min']:.4f}, {smoothness_summary['max']:.4f}]")
            
    except Exception as e:
        print(f"è­¦å‘Šï¼šä¿å­˜åŠ¨ä½œç»Ÿè®¡å€¼åˆ° {action_stats_path} æ—¶å‡ºé”™: {e}")
else:
    print("\nè­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•åŠ¨ä½œæ•°æ®ï¼Œæ— æ³•ç”ŸæˆåŠ¨ä½œç»Ÿè®¡å€¼")

print("\nå…ƒæ•°æ®æ–‡ä»¶ç”Ÿæˆå®Œæ¯•ï¼")

# ç”Ÿæˆ modality.json æ–‡ä»¶
modality_json_path = os.path.join(meta_path, "modality.json")
modality_json_data = {
    "state": {
        "single_arm_eef_xyz": {
            "start": 0,
            "end": 3
        },
        "single_arm_eef_rpy": {
            "start": 3,
            "end": 6
        },
        "gripper": {
            "start": 6,
            "end": 7
        }
    },
    "action": {
        "single_arm_eef_xyz": {
            "start": 0,
            "end": 3
        },
        "single_arm_eef_rpy": {
            "start": 3,
            "end": 6
        },
        "gripper": {
            "start": 6,
            "end": 7
        }
    },
    "video": {
        "webcam": {
            "original_key": "observation.images.webcam"
        },
        "phone": {
            "original_key": "observation.images.phone"
        }
    },
    "annotation": {
        "human.task_description": {
            "original_key": "task_index"
        }
    }
}

try:
    with open(modality_json_path, 'w', encoding='utf-8') as f:
        json.dump(modality_json_data, f, indent=4, ensure_ascii=False)
    print(f"âœ“ modality.json å·²ä¿å­˜åˆ°: {modality_json_path}")
except Exception as e:
    print(f"è­¦å‘Šï¼šä¿å­˜ modality.json åˆ° {modality_json_path} æ—¶å‡ºé”™: {e}")
